2022-01-17 18:00:58,675 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.2.0
TS Home: /home/snoooze/anaconda3/envs/pokemon/lib/python3.8/site-packages
Current directory: /home/snoooze/Git/dtu_mlops/s8_deployment/exercise_files
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 3464 M
Python executable: /home/snoooze/anaconda3/envs/pokemon/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/snoooze/Git/dtu_mlops/s8_deployment/exercise_files/model_store
Initial Models: my_fancy_model=my_fancy_model.mar
Log dir: /home/snoooze/Git/dtu_mlops/s8_deployment/exercise_files/logs
Metrics dir: /home/snoooze/Git/dtu_mlops/s8_deployment/exercise_files/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
2022-01-17 18:00:58,703 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: my_fancy_model.mar
2022-01-17 18:00:59,050 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 634b850f05d5453bb246539a7447de4e
2022-01-17 18:00:59,060 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_fancy_model
2022-01-17 18:00:59,060 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_fancy_model
2022-01-17 18:00:59,060 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_fancy_model loaded.
2022-01-17 18:00:59,060 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_fancy_model, count: 16
2022-01-17 18:00:59,073 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-01-17 18:00:59,219 [INFO ] W-9010-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9010
2022-01-17 18:00:59,221 [INFO ] W-9014-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9014
2022-01-17 18:00:59,221 [INFO ] W-9015-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9015
2022-01-17 18:00:59,220 [INFO ] W-9003-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9003
2022-01-17 18:00:59,220 [INFO ] W-9009-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9009
2022-01-17 18:00:59,220 [INFO ] W-9012-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9012
2022-01-17 18:00:59,220 [INFO ] W-9000-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000
2022-01-17 18:00:59,220 [INFO ] W-9002-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9002
2022-01-17 18:00:59,220 [INFO ] W-9001-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9001
2022-01-17 18:00:59,223 [INFO ] W-9004-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9004
2022-01-17 18:00:59,220 [INFO ] W-9013-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9013
2022-01-17 18:00:59,220 [INFO ] W-9008-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9008
2022-01-17 18:00:59,220 [INFO ] W-9007-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9007
2022-01-17 18:00:59,221 [INFO ] W-9006-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9006
2022-01-17 18:00:59,221 [INFO ] W-9011-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9011
2022-01-17 18:00:59,223 [INFO ] W-9003-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10259
2022-01-17 18:00:59,223 [INFO ] W-9015-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10236
2022-01-17 18:00:59,223 [INFO ] W-9010-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10234
2022-01-17 18:00:59,223 [INFO ] W-9003-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,223 [INFO ] W-9015-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,224 [INFO ] W-9010-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,224 [INFO ] W-9003-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,224 [INFO ] W-9015-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,224 [INFO ] W-9010-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,224 [DEBUG] W-9003-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,224 [INFO ] W-9012-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10250
2022-01-17 18:00:59,224 [INFO ] W-9007-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10233
2022-01-17 18:00:59,225 [INFO ] W-9002-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10255
2022-01-17 18:00:59,225 [DEBUG] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9015-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,224 [DEBUG] W-9010-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9010-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [INFO ] W-9002-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,224 [INFO ] W-9006-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10258
2022-01-17 18:00:59,225 [INFO ] W-9011-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10249
2022-01-17 18:00:59,224 [INFO ] W-9008-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10235
2022-01-17 18:00:59,225 [DEBUG] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [DEBUG] W-9011-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9011-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [INFO ] W-9008-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,224 [INFO ] W-9004-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10252
2022-01-17 18:00:59,224 [INFO ] W-9001-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10257
2022-01-17 18:00:59,224 [INFO ] W-9013-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10256
2022-01-17 18:00:59,225 [DEBUG] W-9004-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,224 [INFO ] W-9014-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10232
2022-01-17 18:00:59,224 [INFO ] W-9009-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10253
2022-01-17 18:00:59,224 [INFO ] W-9000-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10251
2022-01-17 18:00:59,226 [DEBUG] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,226 [DEBUG] W-9013-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9013-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,226 [INFO ] W-9013-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9001-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9004-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9008-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,226 [INFO ] W-9001-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,225 [DEBUG] W-9008-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [INFO ] W-9011-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9006-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9002-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,225 [DEBUG] W-9002-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [DEBUG] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [DEBUG] W-9012-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9012-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,225 [INFO ] W-9007-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,225 [INFO ] W-9012-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,227 [INFO ] W-9006-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,227 [INFO ] W-9011-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,227 [INFO ] W-9005-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9005
2022-01-17 18:00:59,226 [INFO ] W-9004-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,226 [INFO ] W-9013-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,226 [DEBUG] W-9014-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9014-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,226 [INFO ] W-9014-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,226 [DEBUG] W-9000-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,226 [DEBUG] W-9009-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9009-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,226 [INFO ] W-9000-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,226 [INFO ] W-9009-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,228 [INFO ] W-9014-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,228 [INFO ] W-9005-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]10254
2022-01-17 18:00:59,227 [INFO ] W-9007-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,227 [INFO ] W-9012-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,228 [INFO ] W-9005-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
2022-01-17 18:00:59,228 [INFO ] W-9009-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,229 [INFO ] W-9005-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,228 [INFO ] W-9000-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.8.12
2022-01-17 18:00:59,229 [DEBUG] W-9005-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-my_fancy_model_1.0 State change null -> WORKER_STARTED
2022-01-17 18:00:59,229 [INFO ] W-9005-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005
2022-01-17 18:00:59,229 [INFO ] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9007
2022-01-17 18:00:59,229 [INFO ] W-9011-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9011
2022-01-17 18:00:59,229 [INFO ] W-9013-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9013
2022-01-17 18:00:59,229 [INFO ] W-9010-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9010
2022-01-17 18:00:59,229 [INFO ] W-9009-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9009
2022-01-17 18:00:59,229 [INFO ] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-01-17 18:00:59,229 [INFO ] W-9000-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-01-17 18:00:59,229 [INFO ] W-9002-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2022-01-17 18:00:59,229 [INFO ] W-9008-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9008
2022-01-17 18:00:59,229 [INFO ] W-9014-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9014
2022-01-17 18:00:59,229 [INFO ] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9006
2022-01-17 18:00:59,229 [INFO ] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9015
2022-01-17 18:00:59,229 [INFO ] W-9012-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9012
2022-01-17 18:00:59,229 [INFO ] W-9004-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004
2022-01-17 18:00:59,229 [INFO ] W-9003-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2022-01-17 18:00:59,243 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-01-17 18:00:59,243 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-01-17 18:00:59,244 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-01-17 18:00:59,244 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-01-17 18:00:59,245 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-01-17 18:00:59,247 [INFO ] W-9009-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9009.
2022-01-17 18:00:59,247 [INFO ] W-9001-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9001.
2022-01-17 18:00:59,248 [INFO ] W-9000-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.
2022-01-17 18:00:59,248 [INFO ] W-9010-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9010.
2022-01-17 18:00:59,248 [INFO ] W-9011-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9011.
2022-01-17 18:00:59,248 [INFO ] W-9014-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9014.
2022-01-17 18:00:59,248 [INFO ] W-9002-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9002.
2022-01-17 18:00:59,249 [INFO ] W-9005-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9005.
2022-01-17 18:00:59,248 [INFO ] W-9003-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9003.
2022-01-17 18:00:59,249 [INFO ] W-9008-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9008.
2022-01-17 18:00:59,249 [INFO ] W-9015-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9015.
2022-01-17 18:00:59,249 [INFO ] W-9007-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9007.
2022-01-17 18:00:59,249 [INFO ] W-9012-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9012.
2022-01-17 18:00:59,249 [INFO ] W-9006-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9006.
2022-01-17 18:00:59,249 [INFO ] W-9004-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9004.
2022-01-17 18:00:59,250 [INFO ] W-9013-my_fancy_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9013.
2022-01-17 18:01:00,004 [INFO ] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 650
2022-01-17 18:01:00,005 [INFO ] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 651
2022-01-17 18:01:00,005 [INFO ] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 659
2022-01-17 18:01:00,005 [INFO ] W-9010-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 651
2022-01-17 18:01:00,004 [INFO ] W-9008-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 650
2022-01-17 18:01:00,005 [INFO ] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 651
2022-01-17 18:01:00,005 [DEBUG] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [DEBUG] W-9008-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [DEBUG] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [DEBUG] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [DEBUG] W-9010-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9010-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [DEBUG] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9015-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,005 [INFO ] W-9000-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 664
2022-01-17 18:01:00,005 [DEBUG] W-9000-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,008 [INFO ] W-9004-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 667
2022-01-17 18:01:00,008 [INFO ] W-9012-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 660
2022-01-17 18:01:00,008 [DEBUG] W-9004-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,008 [DEBUG] W-9012-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9012-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,009 [INFO ] W-9011-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 654
2022-01-17 18:01:00,009 [INFO ] W-9013-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 655
2022-01-17 18:01:00,009 [DEBUG] W-9011-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9011-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,009 [DEBUG] W-9013-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9013-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,009 [INFO ] W-9009-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 670
2022-01-17 18:01:00,009 [DEBUG] W-9009-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9009-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,012 [INFO ] W-9002-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 664
2022-01-17 18:01:00,012 [DEBUG] W-9002-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,014 [INFO ] W-9014-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 666
2022-01-17 18:01:00,014 [DEBUG] W-9014-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9014-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,016 [INFO ] W-9005-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 662
2022-01-17 18:01:00,016 [DEBUG] W-9005-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:00,017 [INFO ] W-9003-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 663
2022-01-17 18:01:00,017 [DEBUG] W-9003-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-my_fancy_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-01-17 18:01:59,729 [INFO ] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 198
2022-01-17 18:01:59,732 [DEBUG] W-9007-my_fancy_model_1.0 org.pytorch.serve.wlm.Job - Waiting time ns: 185776, Backend time ns: 201042111
2022-01-17 18:02:07,796 [INFO ] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179
2022-01-17 18:02:07,798 [DEBUG] W-9006-my_fancy_model_1.0 org.pytorch.serve.wlm.Job - Waiting time ns: 121819, Backend time ns: 181219515
2022-01-17 18:04:20,149 [INFO ] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 204
2022-01-17 18:04:20,150 [DEBUG] W-9001-my_fancy_model_1.0 org.pytorch.serve.wlm.Job - Waiting time ns: 150824, Backend time ns: 205320394
2022-01-17 18:05:16,928 [INFO ] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 223
2022-01-17 18:05:16,929 [DEBUG] W-9015-my_fancy_model_1.0 org.pytorch.serve.wlm.Job - Waiting time ns: 166941, Backend time ns: 227263091
2022-01-17 18:09:51,860 [INFO ] epollEventLoopGroup-2-2 org.pytorch.serve.ModelServer - Management model server stopped.
2022-01-17 18:09:51,860 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-01-17 18:09:51,863 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Metrics model server stopped.
2022-01-17 18:09:53,893 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
